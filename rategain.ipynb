{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import calendar\n",
        "import csv\n",
        "\n",
        "base_url = \"https://rategain.com/blog/\"\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
        "})\n",
        "\n",
        "blog_data = []\n",
        "\n",
        "# Create a set of month names for easy checking\n",
        "months = {month.lower() for month in calendar.month_name[1:]}\n",
        "\n",
        "# Loop through pages until no more posts are found\n",
        "page_num = 1\n",
        "while True:\n",
        "    url = f\"{base_url}page/{page_num}/\"\n",
        "    response = session.get(url)  # Fetch the specific page URL\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        blog_posts = soup.find_all('div', class_='wrap')  # Update class to the blog post container for images\n",
        "\n",
        "        if not blog_posts:\n",
        "            print(f\"No blog posts found on page {page_num}\")\n",
        "            break  # No more pages to scrape\n",
        "\n",
        "        for post in blog_posts:\n",
        "            image_link = post.find('a', class_='rocket-lazyload')\n",
        "\n",
        "            if image_link and 'data-bg' in image_link.attrs:\n",
        "                image_url = image_link['data-bg']\n",
        "            else:\n",
        "                image_url = \"NaN\"  # Placeholder for missing image\n",
        "\n",
        "            blog_title = post.find_previous('div', class_='content')  # Find the corresponding title based on the structure\n",
        "            if blog_title:\n",
        "                title_element = blog_title.find('h6')  # Assuming blog titles are in <h6> tags\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                else:\n",
        "                    title = \"Title Not Found\"\n",
        "            else:\n",
        "                title = \"Title Not Found\"\n",
        "\n",
        "            blog_date = post.find('div', class_='bd-item')\n",
        "            publication_date = \"Date Not Found\"\n",
        "\n",
        "            if blog_date:\n",
        "                date_icon = blog_date.find('i', class_='material-design-icon-history-clock-button')\n",
        "                if date_icon:\n",
        "                    next_element = date_icon.find_next('span')\n",
        "                    if next_element and any(month in next_element.text.lower() for month in months):\n",
        "                        publication_date = next_element.text.strip()\n",
        "                    else:\n",
        "                        publication_date = \"Date Not Found\"\n",
        "                else:\n",
        "                    publication_date = \"Date Not Found\"\n",
        "            else:\n",
        "                publication_date = \"Date Not Found\"\n",
        "            # Extract likes count\n",
        "            #likes_count = \"Likes Not Found\"\n",
        "            # Extract likes count\n",
        "\n",
        "            likes_tag = post.find('a', class_='zilla-likes')\n",
        "            if likes_tag:\n",
        "                likes_span = likes_tag.find('span')\n",
        "                if likes_span:\n",
        "                     span_text = likes_span.text.strip('\"')\n",
        "\n",
        "                     if span_text:\n",
        "                        likes_count = span_text\n",
        "                     else:\n",
        "                        likes_count=\"no\"\n",
        "                else:\n",
        "                     likes_count=\"no likes\"\n",
        "\n",
        "\n",
        "            blog_data.append({'Blog title': title, 'Blog image URL': image_url, 'Blog date': publication_date,'Blog likes count': likes_count})\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch URL: {url}\")\n",
        "        break\n",
        "\n",
        "    page_num += 1\n",
        "\n",
        "# Save data to a CSV file\n",
        "csv_filename = 'blog_data.csv'\n",
        "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "    fieldnames = ['Blog title', 'Blog image URL', 'Blog date','Blog likes count']\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for data in blog_data:\n",
        "        writer.writerow(data)\n",
        "\n",
        "print(f\"Total blog posts found: {len(blog_data)}\")\n",
        "print(f\"CSV file '{csv_filename}' has been created with the scraped data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkapv5UbYJvp",
        "outputId": "11463560-5cc4-491c-9aaa-395953b5bd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No blog posts found on page 46\n",
            "Total blog posts found: 402\n",
            "CSV file 'blog_data.csv' has been created with the scraped data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPQvQJVncvrf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}